{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N3aid9n44LCI"
      },
      "outputs": [],
      "source": [
        "! pip install streamlit -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkS0o2mroUAD",
        "outputId": "1e42f650-3e58-4f42-d20d-9ef8879305e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytube in /usr/local/lib/python3.10/dist-packages (15.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CgMpu_TdtgwI"
      },
      "outputs": [],
      "source": [
        "# changed client from default ANDROID_MUSIC\n",
        "from pytube.innertube import _default_clients\n",
        "_default_clients[\"ANDROID\"] = _default_clients[\"WEB\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucDAgldOpP08",
        "outputId": "3b34d941-5bd7-4301-d110-8181f96cd5f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import itertools\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YF22krA-pVlk",
        "outputId": "906fb64d-7a67-4ed8-c30c-b9b39decef9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertModel, AutoTokenizer\n",
        "\n",
        "# Load pre-trained DistilBERT model and tokenizer\n",
        "model_name = 'distilbert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name) # DistilBertTokenizer.from_pretrained(model_name)\n",
        "model = DistilBertModel.from_pretrained(model_name)\n",
        "\n",
        "def generate_text_embeddings(texts):\n",
        "    # Tokenize input texts\n",
        "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "    # Generate embeddings\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Get the embeddings from the last hidden layer\n",
        "    embeddings = outputs.last_hidden_state[:, 0, :] # <---CLS # outputs.last_hidden_state.mean(dim=1)  # You can use other aggregation methods\n",
        "\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ciwK82WBpsNx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "dropout = 0.1\n",
        "class ImageProjection(nn.Module):\n",
        "    def __init__(self, image_embedding_size, shared_embedding_size):\n",
        "        super(ImageProjection, self).__init__()\n",
        "        self.image_projection = nn.Linear(image_embedding_size, shared_embedding_size)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.fc = nn.Linear(shared_embedding_size, shared_embedding_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(shared_embedding_size)\n",
        "\n",
        "    def forward(self, image_embeddings):\n",
        "        projected_embeddings = self.image_projection(image_embeddings)\n",
        "\n",
        "        x = self.gelu(projected_embeddings)\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x + projected_embeddings\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        return x # projected_embeddings\n",
        "\n",
        "class TextProjection(nn.Module):\n",
        "    def __init__(self, text_embedding_size, shared_embedding_size):\n",
        "        super(TextProjection, self).__init__()\n",
        "        self.text_projection = nn.Linear(text_embedding_size, shared_embedding_size)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.fc = nn.Linear(shared_embedding_size, shared_embedding_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(shared_embedding_size)\n",
        "\n",
        "    def forward(self, text_embeddings):\n",
        "        projected_embeddings = self.text_projection(text_embeddings)\n",
        "\n",
        "        x = self.gelu(projected_embeddings)\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x + projected_embeddings\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        return x # projected_embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "jgM74__IpsQE"
      },
      "outputs": [],
      "source": [
        "dropout = 0.1\n",
        "\n",
        "def cross_entropy(preds, targets, reduction='none'):\n",
        "    log_softmax = nn.LogSoftmax(dim=-1)\n",
        "    loss = (-targets * log_softmax(preds)).sum(1)\n",
        "    if reduction == \"none\":\n",
        "        return loss\n",
        "    elif reduction == \"mean\":\n",
        "        return loss.mean()\n",
        "\n",
        "temperature_value = torch.nn.Parameter(torch.tensor(1.0910))  # Initialize as a learnable parameter\n",
        "\n",
        "def contrastive_clip_loss_function( text_projection,  image_projection, mode=\"eval\" ):\n",
        "    logits = (text_projection @ image_projection.T) / temperature_value\n",
        "    if mode==\"train\":\n",
        "        images_similarity = image_projection @ image_projection.T\n",
        "        texts_similarity = text_projection @ text_projection.T\n",
        "        targets = F.softmax( (images_similarity + texts_similarity) / 2 * temperature_value, dim=-1 )\n",
        "        texts_loss = cross_entropy(logits, targets, reduction='none')\n",
        "        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
        "        loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n",
        "        return loss.mean()\n",
        "    elif mode==\"eval\":\n",
        "        return logits\n",
        "    else:\n",
        "        print(\"Mention mode\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Ug0L-tKWpsVE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Dc4XaRdFp2G-"
      },
      "outputs": [],
      "source": [
        "\n",
        "shared_embedding_size = 512\n",
        "learning_rate = 5e-4\n",
        "\n",
        "image_embedding_size=1024*2\n",
        "resnet_model = models.resnet50(pretrained=True)\n",
        "resnet_model = nn.Sequential( *list(resnet_model.children())[:-1] )\n",
        "resnet_model.to(device)\n",
        "\n",
        "max_length = 32\n",
        "text_embedding_size = 768\n",
        "model_name = 'distilbert-base-uncased'\n",
        "text_tokenizer = DistilBertTokenizer.from_pretrained(model_name) # .to(device)\n",
        "text_model = DistilBertModel.from_pretrained(model_name ).to(device)\n",
        "\n",
        "image_projector = ImageProjection(image_embedding_size, shared_embedding_size).to(device)\n",
        "text_projector = TextProjection(text_embedding_size, shared_embedding_size).to(device)\n",
        "\n",
        "params = [{\"params\":resnet_model.parameters(), \"lr\":1e-4 }, {\"params\":text_model.parameters(), \"lr\":1e-5},\n",
        "          # {\"params\":image_projector.parameters(), \"lr\": }, {\"params\":text_projector.parameters(), \"lr\":}\n",
        "          {\"params\": itertools.chain( image_projector.parameters(), text_projector.parameters(), [temperature_value] ), \"lr\":1e-3 , \"weight_decay\":1e-3 }\n",
        "         ]\n",
        "optimizer = optim.AdamW( params,  weight_decay=0. )\n",
        "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( optimizer, mode=\"min\", patience=2 , factor=0.8 )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-jYyEoCyp2J3"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuX399wHp2O9",
        "outputId": "57984e0b-afef-4e1e-b044-d975e98c3b76"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ImageProjection(\n",
              "  (image_projection): Linear(in_features=2048, out_features=512, bias=True)\n",
              "  (gelu): GELU(approximate='none')\n",
              "  (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Load the saved model checkpoint\n",
        "checkpoint_path = \"/content/drive/MyDrive/datasets/final_odel_from_Scratch_ResNet_DistilBERT.h5\"\n",
        "checkpoint = model = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
        "\n",
        "\n",
        "# Load the state dictionaries into the models and optimizer\n",
        "resnet_model.load_state_dict(checkpoint['resnet_model_dict'])\n",
        "text_model.load_state_dict(checkpoint['text_model_dict'])\n",
        "image_projector.load_state_dict(checkpoint['image_projector_dict'])\n",
        "text_projector.load_state_dict(checkpoint['text_projector_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "temperature_value_1 = nn.Parameter(torch.tensor(checkpoint['temperature_value']), requires_grad=True)\n",
        "\n",
        "# Set the models to evaluation mode\n",
        "\n",
        "image_projector.eval()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "gpXj40G8rGdA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byePTWBdq9Fk",
        "outputId": "16d0cdcf-9585-4c8e-9a15-d0401e980b33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "h1X363KfrW2t"
      },
      "outputs": [],
      "source": [
        "from pytube import YouTube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "m_fPznFYrW5S"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"AlexZigma/msr-vtt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jSBf-nArW72",
        "outputId": "e64eb610-550d-460c-b73b-5efac86c8d52"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'video_id': 'video6513',\n",
              " 'caption': 'a family is having coversation',\n",
              " 'sen_id': 83560,\n",
              " 'category': 14,\n",
              " 'url': 'https://www.youtube.com/watch?v=A9pM9iOuAzM',\n",
              " 'start time': 116.03,\n",
              " 'end time': 126.21,\n",
              " 'split': 'validate',\n",
              " 'id': 6513,\n",
              " '__index_level_0__': 6128}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "dataset['val'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "eQcxzRNBrW-O"
      },
      "outputs": [],
      "source": [
        "from pytube import YouTube\n",
        "\n",
        "def download_video(video_url, output_filename):\n",
        "    try:\n",
        "        # Choose a video stream with resolution of 360p\n",
        "        streams = YouTube(url=video_url).streams.filter(adaptive=True, subtype=\"mp4\", resolution=\"360p\", only_video=True)\n",
        "\n",
        "        # Check if there is a valid stream\n",
        "        if len(streams) == 0:\n",
        "            raise Exception(\"No suitable stream found for this YouTube video!\")\n",
        "\n",
        "        # Download the video\n",
        "        print(\"Downloading...\")\n",
        "        streams[0].download(filename=output_filename)\n",
        "        print(\"Download completed.\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading video: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "R_tGmUxxrXAg"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "def extract_frames_with_timestamp(video_file):\n",
        "    # Open the video file\n",
        "    cap = cv2.VideoCapture(video_file)\n",
        "\n",
        "    # Initialize lists to store frame data and timestamps\n",
        "    frames_list = []\n",
        "    timestamps_list = []\n",
        "\n",
        "    # Initialize variables for frame extraction\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    interval = int(fps)  # Extract one frame per second\n",
        "\n",
        "    # Initialize frame counter\n",
        "    frame_count = 0\n",
        "\n",
        "    # Read frames until the end of the video\n",
        "    while True:\n",
        "        # Read a frame from the video\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        # Break the loop if no frame is read\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Get the current timestamp (in milliseconds)\n",
        "        timestamp_ms = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
        "\n",
        "        # Convert timestamp to seconds\n",
        "        timestamp_sec = timestamp_ms / 1000.0\n",
        "\n",
        "        # Check if it's time to extract a frame\n",
        "        if frame_count % interval == 0:\n",
        "            # Append frame and timestamp to the lists\n",
        "            frames_list.append(frame)\n",
        "            timestamps_list.append(timestamp_sec)\n",
        "\n",
        "        # Increment frame counter\n",
        "        frame_count += 1\n",
        "\n",
        "    # Close the video file\n",
        "    cap.release()\n",
        "    print(\"Frames extracted:\", len(frames_list))\n",
        "    return frames_list, timestamps_list\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "PzR5eYYhrXDC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "class FrameDataset(Dataset):\n",
        "    def __init__(self, frame_list, image_size=224):\n",
        "        self.frame_list = frame_list\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((image_size, image_size)),  # Resize images to a consistent size\n",
        "            transforms.ToTensor(),  # Convert images to tensors\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize images\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.frame_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        frame = self.frame_list[index]\n",
        "\n",
        "        # Convert NumPy array to PIL Image\n",
        "        frame_pil = Image.fromarray(frame)\n",
        "\n",
        "        # Apply transformations\n",
        "        frame_tensor = self.transform(frame_pil)\n",
        "\n",
        "        return frame_tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "6qFDIK4gsLoX"
      },
      "outputs": [],
      "source": [
        "resnet_model.eval()\n",
        "image_projector.eval()\n",
        "def new_create_image_embeddings(images):\n",
        "    with torch.no_grad():\n",
        "        image_embeddings = resnet_model(images)\n",
        "        image_projection = image_projector(image_embeddings[:,:,0,0])\n",
        "    return image_projection\n",
        "\n",
        "\n",
        "def retrieve_frame(frame_dataset, input_query, n ):\n",
        "    new_image_embeddings_list_train = []\n",
        "\n",
        "    for index in tqdm(range(len( frame_dataset ))):\n",
        "        images = frame_dataset[index]\n",
        "        images = images.to(device)\n",
        "        image_projection = new_create_image_embeddings(images.unsqueeze(0))\n",
        "        new_image_embeddings_list_train.append( image_projection[0] )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        inputs = tokenizer(input_query, return_tensors='pt', padding=\"max_length\", max_length=max_length, truncation=True)\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = text_model(**inputs)\n",
        "        text_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "        text_projection = text_projector(text_embeddings)\n",
        "\n",
        "    similarity_scores_list = []\n",
        "    for index in tqdm(range(len(new_image_embeddings_list_train))):\n",
        "        score = torch.dot( text_projection[0], new_image_embeddings_list_train[index] )\n",
        "        similarity_scores_list.append( score.cpu().numpy() )\n",
        "\n",
        "    max_indexes = np.array(similarity_scores_list).argsort()[-n:][::-1]\n",
        "\n",
        "    return max_indexes\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "S8D5RJWdsLqp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_frames_matching_query(video_url, query):\n",
        "    # Download the video\n",
        "    output_filename = \"video.mp4\"\n",
        "    if not download_video(video_url, output_filename):\n",
        "        print(\"Failed to download video.\")\n",
        "        return\n",
        "\n",
        "    # Extract frames from the downloaded video\n",
        "    video_file = \"video.mp4\"\n",
        "    frames_list, timestamps_list = extract_frames_with_timestamp(video_file)\n",
        "\n",
        "    # Retrieve frames matching the query\n",
        "    frame_dataset = FrameDataset(frames_list)\n",
        "    indexes = retrieve_frame(frame_dataset, query, n=5)\n",
        "    selected_frames = [frames_list[i] for i in indexes]\n",
        "    selected_timestamps = [timestamps_list[i] for i in indexes]\n",
        "\n",
        "    # Display the frames matching the query and their timestamps\n",
        "    print(f\"Frames matching the query '{query}':\")\n",
        "    for frame, timestamp in zip(selected_frames, selected_timestamps):\n",
        "        print(f\"Timestamp: {timestamp}\")\n",
        "        # Display the frame as an image using matplotlib\n",
        "        plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        plt.axis('off')  # Turn off axis\n",
        "        plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-foZnbptGxZ",
        "outputId": "8d995576-524f-46d2-9089-576e5fdd7656"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'video_id': 'video6523',\n",
              " 'caption': 'a man wearing a hoody dress removes the headset from his head',\n",
              " 'sen_id': 108940,\n",
              " 'category': 7,\n",
              " 'url': 'https://www.youtube.com/watch?v=ylSh0dOsd8Q',\n",
              " 'start time': 113.04,\n",
              " 'end time': 125.09,\n",
              " 'split': 'validate',\n",
              " 'id': 6523,\n",
              " '__index_level_0__': 6139}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "dataset['val'][10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_yf2Hk8sLtq",
        "outputId": "af32f6e2-abc3-4bce-acc4-f5f13810e25f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "Download completed.\n",
            "Frames extracted: 359\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 47/359 [00:29<01:44,  2.99it/s]"
          ]
        }
      ],
      "source": [
        "first_row = dataset['val'][10]\n",
        "video_url = first_row['url']\n",
        "start_time = first_row['start time']\n",
        "end_time = first_row['end time']\n",
        "query = first_row['caption']\n",
        "# Call the function\n",
        "display_frames_matching_query(video_url, query)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_url= \"\"\n",
        "query=\"\"\n",
        "display_frames_matching_query(video_url, query)"
      ],
      "metadata": {
        "id": "4DFSCJXEaLyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLlQuAk9s3Zn",
        "outputId": "c21261d8-5192-4f00-b180-7b48b4d4b570"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.70.223.100\n"
          ]
        }
      ],
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sUGSCBo1Pqd",
        "outputId": "6d4e57c8-575e-4c5e-c562-4341c4380a10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from pytube.innertube import _default_clients\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.models import resnet50\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import psutil\n",
        "_default_clients[\"ANDROID\"] = _default_clients[\"WEB\"]\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from torchvision import models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pytube import YouTube\n",
        "from datasets import load_dataset\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import itertools\n",
        "import cv2\n",
        "import os\n",
        "import cv2\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from torchvision.models import resnet50\n",
        "from tqdm import tqdm\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "\n",
        "from transformers import DistilBertTokenizer, DistilBertModel, AutoTokenizer\n",
        "\n",
        "# Load pre-trained DistilBERT model and tokenizer\n",
        "model_name = 'distilbert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name) # DistilBertTokenizer.from_pretrained(model_name)\n",
        "model = DistilBertModel.from_pretrained(model_name)\n",
        "\n",
        "def generate_text_embeddings(texts):\n",
        "    start_time = time.time()  # Start time for text embedding\n",
        "    # Tokenize input texts\n",
        "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
        "    # Generate embeddings\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    # Get the embeddings from the last hidden layer\n",
        "    embeddings = outputs.last_hidden_state[:, 0, :]  # <---CLS # outputs.last_hidden_state.mean(dim=1)  # You can use other aggregation methods\n",
        "    end_time = time.time()  # End time for text embedding\n",
        "    elapsed_time = end_time - start_time  # Calculate elapsed time\n",
        "    return embeddings, elapsed_time\n",
        "\n",
        "\n",
        "# Define ResNet-BERT models\n",
        "class ImageProjection(nn.Module):\n",
        "    def __init__(self, image_embedding_size, shared_embedding_size):\n",
        "        super(ImageProjection, self).__init__()\n",
        "        self.image_projection = nn.Linear(image_embedding_size, shared_embedding_size)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.fc = nn.Linear(shared_embedding_size, shared_embedding_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(shared_embedding_size)\n",
        "\n",
        "    def forward(self, image_embeddings):\n",
        "        projected_embeddings = self.image_projection(image_embeddings)\n",
        "\n",
        "        x = self.gelu(projected_embeddings)\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x + projected_embeddings\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class TextProjection(nn.Module):\n",
        "    def __init__(self, text_embedding_size, shared_embedding_size):\n",
        "        super(TextProjection, self).__init__()\n",
        "        self.text_projection = nn.Linear(text_embedding_size, shared_embedding_size)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.fc = nn.Linear(shared_embedding_size, shared_embedding_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(shared_embedding_size)\n",
        "\n",
        "    def forward(self, text_embeddings):\n",
        "        projected_embeddings = self.text_projection(text_embeddings)\n",
        "\n",
        "        x = self.gelu(projected_embeddings)\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x + projected_embeddings\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "dropout = 0.1\n",
        "\n",
        "def cross_entropy(preds, targets, reduction='none'):\n",
        "    log_softmax = nn.LogSoftmax(dim=-1)\n",
        "    loss = (-targets * log_softmax(preds)).sum(1)\n",
        "    if reduction == \"none\":\n",
        "        return loss\n",
        "    elif reduction == \"mean\":\n",
        "        return loss.mean()\n",
        "\n",
        "temperature_value = torch.nn.Parameter(torch.tensor(1.0910))  # Initialize as a learnable parameter\n",
        "\n",
        "def contrastive_clip_loss_function( text_projection,  image_projection, mode=\"eval\" ):\n",
        "    logits = (text_projection @ image_projection.T) / temperature_value\n",
        "    if mode==\"train\":\n",
        "        images_similarity = image_projection @ image_projection.T\n",
        "        texts_similarity = text_projection @ text_projection.T\n",
        "        targets = F.softmax( (images_similarity + texts_similarity) / 2 * temperature_value, dim=-1 )\n",
        "        texts_loss = cross_entropy(logits, targets, reduction='none')\n",
        "        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
        "        loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n",
        "        return loss.mean()\n",
        "    elif mode==\"eval\":\n",
        "        return logits\n",
        "    else:\n",
        "        print(\"Mention mode\")\n",
        "        return None\n",
        "\n",
        "\n",
        "shared_embedding_size = 512\n",
        "learning_rate = 5e-4\n",
        "\n",
        "image_embedding_size=1024*2\n",
        "resnet_model = models.resnet50(pretrained=True)\n",
        "resnet_model = nn.Sequential( *list(resnet_model.children())[:-1] )\n",
        "resnet_model.to(device)\n",
        "\n",
        "max_length = 32\n",
        "text_embedding_size = 768\n",
        "model_name = 'distilbert-base-uncased'\n",
        "text_tokenizer = DistilBertTokenizer.from_pretrained(model_name) # .to(device)\n",
        "text_model = DistilBertModel.from_pretrained(model_name ).to(device)\n",
        "\n",
        "image_projector = ImageProjection(image_embedding_size, shared_embedding_size).to(device)\n",
        "text_projector = TextProjection(text_embedding_size, shared_embedding_size).to(device)\n",
        "\n",
        "params = [{\"params\":resnet_model.parameters(), \"lr\":1e-4 }, {\"params\":text_model.parameters(), \"lr\":1e-5},\n",
        "          # {\"params\":image_projector.parameters(), \"lr\": }, {\"params\":text_projector.parameters(), \"lr\":}\n",
        "          {\"params\": itertools.chain( image_projector.parameters(), text_projector.parameters(), [temperature_value] ), \"lr\":1e-3 , \"weight_decay\":1e-3 }\n",
        "         ]\n",
        "optimizer = optim.AdamW( params,  weight_decay=0. )\n",
        "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( optimizer, mode=\"min\", patience=2 , factor=0.8 )\n",
        "\n",
        "# Load the saved model checkpoint\n",
        "checkpoint_path = \"/content/drive/MyDrive/datasets/final_odel_from_Scratch_ResNet_DistilBERT.h5\"\n",
        "checkpoint = model = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
        "\n",
        "\n",
        "# Load the state dictionaries into the models and optimizer\n",
        "resnet_model.load_state_dict(checkpoint['resnet_model_dict'])\n",
        "text_model.load_state_dict(checkpoint['text_model_dict'])\n",
        "image_projector.load_state_dict(checkpoint['image_projector_dict'])\n",
        "text_projector.load_state_dict(checkpoint['text_projector_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "# Set the models to evaluation mode\n",
        "\n",
        "image_projector.eval()\n",
        "resnet_model.eval()\n",
        "text_model.eval()\n",
        "text_projector.eval()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "dataset = load_dataset(\"AlexZigma/msr-vtt\")\n",
        "\n",
        "\n",
        "def download_video(video_url, output_filename):\n",
        "    try:\n",
        "        # Choose a video stream with resolution of 360p\n",
        "        streams = YouTube(url=video_url).streams.filter(adaptive=True, subtype=\"mp4\", resolution=\"360p\", only_video=True)\n",
        "\n",
        "        # Check if there is a valid stream\n",
        "        if len(streams) == 0:\n",
        "            raise Exception(\"No suitable stream found for this YouTube video!\")\n",
        "\n",
        "        # Download the video\n",
        "        print(\"Downloading...\")\n",
        "        streams[0].download(filename=output_filename)\n",
        "        print(\"Download completed.\")\n",
        "        st.write(\"Download completed.\")\n",
        "        # st.write(\"Frames extracted:\", len(frames_list))\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading video: {e}\")\n",
        "        st.write(f\"Error downloading video: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "class FrameDataset(Dataset):\n",
        "    def __init__(self, frame_list, image_size=224):\n",
        "        self.frame_list = frame_list\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((image_size, image_size)),  # Resize images to a consistent size\n",
        "            transforms.ToTensor(),  # Convert images to tensors\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize images\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.frame_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        frame = self.frame_list[index]\n",
        "\n",
        "        # Convert NumPy array to PIL Image\n",
        "        frame_pil = Image.fromarray(frame)\n",
        "\n",
        "        # Apply transformations\n",
        "        frame_tensor = self.transform(frame_pil)\n",
        "\n",
        "        return frame_tensor\n",
        "\n",
        "def extract_frames_with_timestamp(video_file):\n",
        "    # Open the video file\n",
        "    cap = cv2.VideoCapture(video_file)\n",
        "\n",
        "    # Initialize lists to store frame data and timestamps\n",
        "    frames_list = []\n",
        "    timestamps_list = []\n",
        "\n",
        "    # Initialize variables for frame extraction\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    interval = int(fps)  # Extract one frame per second\n",
        "\n",
        "    # Initialize frame counter\n",
        "    frame_count = 0\n",
        "\n",
        "    # Read frames until the end of the video\n",
        "    while True:\n",
        "        # Read a frame from the video\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        # Break the loop if no frame is read\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Get the current timestamp (in milliseconds)\n",
        "        timestamp_ms = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
        "\n",
        "        # Convert timestamp to seconds\n",
        "        timestamp_sec = timestamp_ms / 1000.0\n",
        "\n",
        "        # Check if it's time to extract a frame\n",
        "        if frame_count % interval == 0:\n",
        "            # Append frame and timestamp to the lists\n",
        "            frames_list.append(frame)\n",
        "            timestamps_list.append(timestamp_sec)\n",
        "\n",
        "        # Increment frame counter\n",
        "        frame_count += 1\n",
        "\n",
        "    # Close the video file\n",
        "    cap.release()\n",
        "    print(\"Frames extracted:\", len(frames_list))\n",
        "    st.write(\"Frames extracted:\", len(frames_list))\n",
        "    return frames_list, timestamps_list\n",
        "\n",
        "\n",
        "\n",
        "# Define function to retrieve frames matching the query\n",
        "def new_create_image_embeddings(images):\n",
        "    # start_time = time.time()  # Start time for image embedding\n",
        "    with torch.no_grad():\n",
        "        image_embeddings = resnet_model(images)\n",
        "        image_projection = image_projector(image_embeddings[:,:,0,0])\n",
        "    # end_time = time.time()  # End time for image embedding\n",
        "    # elapsed_time = end_time - start_time  # Calculate elapsed time\n",
        "    return image_projection\n",
        "\n",
        "\n",
        "\n",
        "def retrieve_frame(frame_dataset, input_query, n ):\n",
        "    new_image_embeddings_list_train = []\n",
        "    print('image embedding')\n",
        "    start_time = time.time()\n",
        "    for index in tqdm(range(len( frame_dataset ))):\n",
        "        images = frame_dataset[index]\n",
        "        images = images.to(device)\n",
        "        image_projection = new_create_image_embeddings(images.unsqueeze(0))\n",
        "        new_image_embeddings_list_train.append(image_projection[0])\n",
        "    end_time = time.time()  # End time for image embedding\n",
        "    elapsed_time = end_time - start_time  # Calculate elapsed time\n",
        "    print(f\"Time taken for image embedding : {elapsed_time:.2f} seconds\")\n",
        "    print('text embedding')\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        inputs = tokenizer(input_query, return_tensors='pt', padding=\"max_length\", max_length=max_length, truncation=True)\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = text_model(**inputs)\n",
        "        text_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "        text_projection = text_projector(text_embeddings)\n",
        "    end_time = time.time()  # End time for image embedding\n",
        "    elapsed_time = end_time - start_time  # Calculate elapsed timeprint(f\"Time taken for text embedding: {text_elapsed_time:.2f} seconds\")\n",
        "    print('similarity calculate')\n",
        "    similarity_scores_list = []\n",
        "    for index in tqdm(range(len(new_image_embeddings_list_train))):\n",
        "        score = torch.dot( text_projection[0], new_image_embeddings_list_train[index] )\n",
        "        similarity_scores_list.append( score.cpu().numpy() )\n",
        "\n",
        "    max_indexes = np.array(similarity_scores_list).argsort()[-n:][::-1]\n",
        "\n",
        "    return max_indexes\n",
        "# Add performance metrics function\n",
        "def get_performance_metrics(frames_processed, start_time, end_time, download_start_time, download_end_time):\n",
        "    # Calculate elapsed time for extraction\n",
        "    extraction_time = end_time - start_time\n",
        "\n",
        "    # Calculate elapsed time for downloading\n",
        "    download_time = download_end_time - download_start_time\n",
        "\n",
        "    # Calculate downloading speed (MB/s)\n",
        "    if download_time > 0:\n",
        "        download_speed = (os.path.getsize(video_filename) / 1024 ** 2) / download_time\n",
        "        st.write(f\"Downloading Speed: {download_speed} MB/s\")\n",
        "\n",
        "    # Calculate extraction speed (frames/s)\n",
        "    if extraction_time > 0:\n",
        "        extraction_speed = frames_processed / extraction_time\n",
        "        st.write(f\"Extraction Speed: {extraction_speed} frames/s\")\n",
        "\n",
        "    # CPU usage\n",
        "    cpu_percent = psutil.cpu_percent()\n",
        "    st.write(f\"CPU Usage: {cpu_percent}%\")\n",
        "\n",
        "    # Memory usage\n",
        "    memory = psutil.virtual_memory()\n",
        "    memory_usage = memory.used / 1024 ** 2  # Convert to MB\n",
        "    st.write(f\"Memory Usage: {memory_usage} MB\")\n",
        "\n",
        "    # Frames processed\n",
        "    st.write(f\"Frames Processed: {frames_processed}\")\n",
        "\n",
        "    # Calculate frames per second (FPS)\n",
        "    if extraction_time > 0:\n",
        "        fps = frames_processed / extraction_time\n",
        "        st.write(f\"Frames per Second (FPS): {fps}\")\n",
        "\n",
        "\n",
        "# Streamlit App\n",
        "st.title(\"Video Frame Extractor\")\n",
        "\n",
        "# Option to input YouTube link and query\n",
        "video_url = st.text_input(\"Enter YouTube Video Link\")\n",
        "query = st.text_input(\"Enter Query\")\n",
        "\n",
        "# if st.button(\"Calculate Image Embeddings\"):\n",
        "#     if video_url:\n",
        "#         download_start_time = time.time()\n",
        "\n",
        "#         # Download video\n",
        "#         video_filename = \"video.mp4\"\n",
        "#         if download_video(video_url, video_filename):\n",
        "#             # End download time\n",
        "#             download_end_time = time.time()\n",
        "\n",
        "#             # Display downloaded video\n",
        "#             st.video(video_filename)\n",
        "\n",
        "#             # Extract frames\n",
        "#             frames, timestamps = extract_frames_with_timestamp(video_filename)\n",
        "#             frame_dataset = FrameDataset(frames)\n",
        "\n",
        "#             # Calculate image embeddings\n",
        "#             image_embeddings = new_create_image_embeddings(frame_dataset)\n",
        "\n",
        "#             # Save image embeddings for later use\n",
        "#             st.cache(image_embeddings, allow_output_mutation=True, show_spinner=False)\n",
        "\n",
        "#             st.write(\"Image embeddings calculated successfully.\")\n",
        "\n",
        "#     else:\n",
        "#         st.write(\"Please enter YouTube video link.\")\n",
        "\n",
        "# if st.button(\"Extract Frames\"):\n",
        "#     if query:\n",
        "#         # Retrieve cached image embeddings\n",
        "#         image_embeddings = st.cache(image_embeddings, allow_output_mutation=True)\n",
        "\n",
        "#         # Start extraction time\n",
        "#         start_time = time.time()\n",
        "\n",
        "#         # Retrieve frames matching query\n",
        "#         selected_indexes = retrieve_frame(frame_dataset, query, n=5)\n",
        "#         selected_frames = [frames[i] for i in selected_indexes]\n",
        "#         selected_timestamps = [timestamps[i] for i in selected_indexes]\n",
        "\n",
        "#         # End extraction time\n",
        "#         end_time = time.time()\n",
        "\n",
        "#         # Display selected frames\n",
        "#         for frame, timestamp in zip(selected_frames, selected_timestamps):\n",
        "#             st.write(f\"Timestamp: {timestamp}\")\n",
        "#             st.image(frame, use_column_width=True)\n",
        "\n",
        "#         # Display video with marked timestamps\n",
        "#         st.video(video_filename, start_time=timestamps[selected_indexes[0]])\n",
        "#         for timestamp in selected_timestamps:\n",
        "#             st.video(video_filename, start_time=timestamp)\n",
        "\n",
        "#         # Performance metrics\n",
        "#         get_performance_metrics(len(frames), start_time, end_time, download_start_time, download_end_time)\n",
        "\n",
        "#     else:\n",
        "#         st.write(\"Please enter query.\")\n",
        "\n",
        "\n",
        "if st.button(\"Extract Frames\"):\n",
        "    if video_url and query:\n",
        "        download_start_time = time.time()\n",
        "\n",
        "        # Download video\n",
        "        video_filename = \"video.mp4\"\n",
        "        if download_video(video_url, video_filename):\n",
        "            # End download time\n",
        "            download_end_time = time.time()\n",
        "\n",
        "            # Display downloaded video\n",
        "            st.video(video_filename)\n",
        "\n",
        "            # Extract frames\n",
        "            frames, timestamps = extract_frames_with_timestamp(video_filename)\n",
        "            frame_dataset = FrameDataset(frames)\n",
        "\n",
        "            # Start extraction time\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Retrieve frames matching query\n",
        "            selected_indexes = retrieve_frame(frame_dataset, query, n=5)\n",
        "            selected_frames = [frames[i] for i in selected_indexes]\n",
        "            selected_timestamps = [timestamps[i] for i in selected_indexes]\n",
        "            print('step')\n",
        "            # End extraction time\n",
        "            end_time = time.time()\n",
        "            progress_bar = st.progress(0)\n",
        "            num_frames = len(frames)\n",
        "            for frame, timestamp in zip(selected_frames, selected_timestamps):\n",
        "                st.write(f\"Timestamp: {timestamp}\")\n",
        "                st.image(frame, use_column_width=True)\n",
        "\n",
        "\n",
        "\n",
        "              # Redisplay video with marked timestamps on progress bar\n",
        "            st.video(video_filename, start_time=timestamps[selected_indexes[0]])\n",
        "            for timestamp in selected_timestamps:\n",
        "                st.video(video_filename, start_time=timestamp)\n",
        "\n",
        "\n",
        "\n",
        "            # Performance metrics\n",
        "            get_performance_metrics(len(frames), start_time, end_time, download_start_time, download_end_time)\n",
        "\n",
        "    else:\n",
        "        st.write(\"Please enter YouTube video link and query.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sODmucgm4Vg8",
        "outputId": "62612803-491e-49fd-cefa-103b11bc3518"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 5.106s\n",
            "your url is: https://wet-ducks-hide.loca.lt\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.70.223.100:8501\u001b[0m\n",
            "\u001b[0m\n",
            "cpu\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading readme: 100% 734/734 [00:00<00:00, 3.75MB/s]\n",
            "Downloading data: 100% 553k/553k [00:00<00:00, 1.60MB/s]\n",
            "Downloading data: 100% 44.8k/44.8k [00:00<00:00, 265kB/s]\n",
            "Generating train split: 100% 6513/6513 [00:00<00:00, 130939.44 examples/s]\n",
            "Generating val split: 100% 497/497 [00:00<00:00, 108503.49 examples/s]\n",
            "cpu\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "cpu\n",
            "cpu\n",
            "Downloading...\n",
            "Download completed.\n",
            "Frames extracted: 338\n",
            "image embedding\n",
            "100% 338/338 [01:15<00:00,  4.45it/s]\n",
            "Time taken for image embedding : 75.96 seconds\n",
            "text embedding\n",
            "similarity calculate\n",
            "100% 338/338 [00:00<00:00, 93139.40it/s]\n",
            "step\n",
            "cpu\n",
            "cpu\n",
            "cpu\n",
            "Downloading...\n",
            "Download completed.\n",
            "Frames extracted: 1511\n",
            "image embedding\n",
            "100% 1511/1511 [05:52<00:00,  4.29it/s]\n",
            "Time taken for image embedding : 352.16 seconds\n",
            "text embedding\n",
            "similarity calculate\n",
            "100% 1511/1511 [00:00<00:00, 58996.07it/s]\n",
            "step\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "Exception in callback _HandlerDelegate.execute.<locals>.<lambda>(<Task cancell.../web.py:1742>>) at /usr/local/lib/python3.10/dist-packages/tornado/web.py:2434\n",
            "handle: <Handle _HandlerDelegate.execute.<locals>.<lambda>(<Task cancell.../web.py:1742>>) at /usr/local/lib/python3.10/dist-packages/tornado/web.py:2434>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/web.py\", line 1786, in _execute\n",
            "    result = await result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/web.py\", line 2756, in get\n",
            "    await self.flush()\n",
            "asyncio.exceptions.CancelledError\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/web.py\", line 2434, in <lambda>\n",
            "    fut.add_done_callback(lambda f: f.result())\n",
            "asyncio.exceptions.CancelledError\n",
            "Exception in callback _HandlerDelegate.execute.<locals>.<lambda>(<Task cancell.../web.py:1742>>) at /usr/local/lib/python3.10/dist-packages/tornado/web.py:2434\n",
            "handle: <Handle _HandlerDelegate.execute.<locals>.<lambda>(<Task cancell.../web.py:1742>>) at /usr/local/lib/python3.10/dist-packages/tornado/web.py:2434>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/web.py\", line 1786, in _execute\n",
            "    result = await result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/web.py\", line 2756, in get\n",
            "    await self.flush()\n",
            "asyncio.exceptions.CancelledError\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/web.py\", line 2434, in <lambda>\n",
            "    fut.add_done_callback(lambda f: f.result())\n",
            "asyncio.exceptions.CancelledError\n",
            "Exception in callback _HandlerDelegate.execute.<locals>.<lambda>(<Task cancell.../web.py:1742>>) at /usr/local/lib/python3.10/dist-packages/tornado/web.py:2434\n",
            "handle: <Handle _HandlerDelegate.execute.<locals>.<lambda>(<Task cancell.../web.py:1742>>) at /usr/local/lib/python3.10/dist-packages/tornado/web.py:2434>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/web.py\", line 1786, in _execute\n",
            "    result = await result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/web.py\", line 2756, in get\n",
            "    await self.flush()\n",
            "asyncio.exceptions.CancelledError\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/web.py\", line 2434, in <lambda>\n",
            "    fut.add_done_callback(lambda f: f.result())\n",
            "asyncio.exceptions.CancelledError\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "! streamlit run app.py & npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}