# Video-Frame-and-Timestamp-retrieval-based-on-Text-Query

This project aims to create a reliable system for content-based retrieval that can precisely retrieve video frames using textual queries. Our goal is to use deep learning techniques to create a semantic space and map both textual queries and video frames in the same space where the similarity between text and images can be measured directly. 

We use Resnet50 and DistilBERT as our image encoder and text encoder respectively. In addition, we used a loss function based on contrastive learning to maximize the alignment of modalities inside a common embedding space. Model Performance testing on Flickr datasets and benchmark dataset MSRVTT was done to calculate recall and other performance metrics. Lastly, a streamlit application that accepts YouTube video links, and text queries allows for testing demos in a real-world scenario. We have incorporated contrastive cross-entropy loss calculation thus obtaining positive-negative pair similarities. Contrastive cross entropy loss enhances model performance by encouraging similar embeddings for similar inputs and dissimilar embeddings for distinct inputs, facilitating better representation learning in tasks like similarity comparison or clustering. Its utility lies in its ability to effectively guide the model towards learning meaningful embeddings by penalizing the distance between similar pairs while pushing apart dissimilar pairs in the embedding space.
